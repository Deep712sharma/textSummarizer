{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1175c9bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import chdir, getcwd, listdir, makedirs\n",
    "from os.path import join,splitext\n",
    "from docx import Document\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import fitz\n",
    "import pytesseract\n",
    "from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c93361c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_dir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcee14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Folder Path: C:\\Users\\acer\\Desktop\\test\n"
     ]
    }
   ],
   "source": [
    "# Access a specific folder within the current directory\n",
    "target_folder = join(current_dir, r\"C:\\Users\\acer\\Desktop\\test\")\n",
    "print(\"Target Folder Path:\", target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d080e258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Target Folder: ['abc.pdf', 'Data Analytics assignment 1questions - Copy (2).docx', 'file-sample_100kB.docx', 'file-sample_150kB.pdf', 'file-sample_500kB.docx']\n"
     ]
    }
   ],
   "source": [
    "# List files and subdirectories within a folder\n",
    "file_list = listdir(target_folder)\n",
    "print(\"Files in Target Folder:\", file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54c7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7836a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28516314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Collection; The collection of raw data\n",
      "is the first step of the data processing cycle, The type\n",
      "of raw data collected has a huge impact on the output\n",
      "produced, Hence, raw data should be gathered from\n",
      "defined and accurate sources so that the subsequent\n",
      "findings are valid and usable, Raw data can include\n",
      "monetary figures, Website cookies, profit/loss\n",
      "statements of a company, user behavior, ete,\n",
      "\n",
      "Step 2: Preparations Data preparation or data\n",
      "cleaning is the process of sorting and filtering the\n",
      "raw data to remove unnecessary and inaccurate\n",
      "data, Raw data is checked for errors, duplication,\n",
      "miscalculations or missing data, and transformed into\n",
      "a suitable form for further analysis and processing,\n",
      "This is done to ensure that only the highest quality\n",
      "data is fed into the processing, unit,\n",
      "\n",
      "Step 3: Input: In this step, the raw data is\n",
      "converted into machine readable form and fed into the\n",
      "processing unit, This can be in the form of data entry\n",
      "through a keyboard, scanner or any other input source,\n",
      "\n",
      "Step 4: Data Processing; In this step, the raw\n",
      "data is subjected to various data processing methods\n",
      "using machine learning and artificial intelligence\n",
      "algorithms to generate a desirable output, This step\n",
      "may vary slightly from process-to-process depending\n",
      "on the source of data being processed (data lakes,\n",
      "online databases, connected devices, etc.) and the\n",
      "intended use of the output.\n",
      "\n",
      " Data Integration: This process is used when\n",
      "data is gathered from various data sources and data\n",
      "are combined to form consistent data, This consistent\n",
      "data after performing data cleaning is used for Data\n",
      "Preparation and analysis, ‘\n",
      "\n",
      "Data Transformation: This step is used to\n",
      "convert the raw data into a specified format according\n",
      "to the need of the model, The options for the\n",
      "transformation of data are given below:\n",
      "\n",
      "Normalization: In this method, numerical data\n",
      "is converted into the specified range, i.e., between\n",
      "0 and one so that sealing of data can be performed,\n",
      "\n",
      "Aggregation: Generalization: In this case, lower level\n",
      "attributes are converted to a higher standard,\n",
      "\n",
      "Data Reduction: After the transformation and\n",
      "scaling of data duplication, i,¢., redundancy within\n",
      "the data is removed and efficiently organize the data\n",
      "during Data Preparation.\n",
      "\n",
      "a\n",
      "\n",
      "Data\n",
      "integrati\n",
      "»\n",
      "Ly\n",
      "\n",
      "ion\n",
      "un &-\n",
      "Pin 4\n",
      "Neen A\n",
      "\n",
      "Tasks of Data Wrangling: The tasks of Data\n",
      "wrangling are described below:\n",
      "\n",
      " “If you want to create an efficient ETL pipeline\n",
      "\n",
      "ae ans : e be: itiful data\n",
      "\n",
      "me mn\n",
      "\n",
      "Data wrangling is the process of progrg\n",
      "mmatically transforming data into a format tha\n",
      "makes it easier to work with, This might mea)\n",
      "modifying all of the values ina viven column ing\n",
      "certain way, or merging multiple columns togethe;\n",
      "The necessity for data wrangling is often a by-produg\n",
      "of poorly collected or presented data, Data that }\n",
      "entered manually by humans is typically fraught with\n",
      "errors: data collected from websites is often Optimizee\n",
      "to be displayed on websites, not to be sorted ang\n",
      "aggregated. Data volumes will continue to grow\n",
      "exponentially as new data sources and systems come\n",
      "online, delivering upon the promise for a deeper\n",
      "understanding of what makes business move forward,\n",
      "Because big data remains such a popular buzzword,\n",
      "many view data in only this form, the piles and piles\n",
      "of stored data that data analysts and scientists must\n",
      "dig into in order to learn about their Surroundings, The\n",
      "future is not so much about how much data businesses\n",
      "can accumulate, but how they can best absorb it in\n",
      "forms that provide them the knowledge they seek...\n",
      "\n",
      "Q.5. a\n",
      "\n",
      "Data integration architects develop data -\n",
      "integration software programs and data integration\n",
      "platforms that facilitate an automated data integration\n",
      "process for connecting and routing data from source\n",
      "systems to target systems. The data preparation process us\n",
      "Standardizing data ats,\n",
      "\n",
      "‘1c\n",
      "\n",
      "Data Preparation is the process of collecting,\n",
      "cleaning, and consolidating data into one file or data\n",
      "table, primarily for use in analysis.\n",
      "\n",
      " [49]\n",
      "90S a a aaa am ld sa td\n",
      "\n",
      "For example, we have data containing 30\n",
      "attributes where two attributes are used to compute\n",
      "another attribute, and that computed feature is used\n",
      "for further analysis, In this way, the data could be\n",
      "changed according to the requirement of the applied\n",
      "model, and Data Preparation can be effective,\n",
      "\n",
      "Tasks of Data Preparation: Different steps\n",
      "are Involved in Data Preprocessing, These sleps are\n",
      "described below:\n",
      "\n",
      " Extraction and Parsing: In the modern ELT\n",
      "process, data ingestion begins with extracting\n",
      "information from a data source, followed by copying\n",
      "the data to its destination: Initial transformations are\n",
      "focused on shaping the format and structure of data\n",
      "to ensure its compatibility with both the destination\n",
      "system and the data already there. Enterprise data integration feeds\n",
      "integrated data into data warehouses or virtual\n",
      "data ‘integration architecture to support enterprise\n",
      "reporting, business intelligence (BI data integration),\n",
      "and advanced analytics.\n",
      "\n",
      " Manual Data Integration: Manual data\n",
      "integration is the process of manually collecting\n",
      "all data from a data source and then copying it to\n",
      "a data warehouse. Anenterprise can choose among a variety of ETL\n",
      "tools that automate the process of data transform ation’\n",
      "Data analysts, data engineers, and data scientists\n",
      "also transform data using scripting languages sah\n",
      "as Python or domain-specific langsuages like SQL.\n",
      "\n",
      "Q.14. * Inconsistent data: The presence of inconsis-\n",
      "stencies are due to the reasons such that\n",
      "existence of duplication within data, human\n",
      "data entry, containing mistakes in codes or\n",
      "names, i.e., violation of data constraints and\n",
      "much more necessitate Data Preparation and\n",
      "analysis.\n",
      "\n",
      " A data lake can include structured\n",
      "data from relational databases (rows and columns),\n",
      "semistructured data (CSV, logs, XML, JSON),\n",
      "unstructured data (emails, documents, PDFs) and\n",
      "binary data (images, audio, video). The primary task of the Data\n",
      "Wrangling method is to manage the newly generated\n",
      "data from various sources for the analysis process\n",
      "whereas the goal of ETL is to extract, transform\n",
      "and load the data into the central enterprise Data\n",
      "Warehouse for performing analysis process using\n",
      "business applications.\n",
      "\n",
      " Used for large amounts\n",
      "of data, :\n",
      "Eg: Payroll system\n",
      "\n",
      "Real-time Datu is processed within seconds\n",
      "\n",
      "Processing when the input is given, Used for\n",
      "small amounts of data,\n",
      "Eg: Withdrawing money from\n",
      "ATM\n",
      "\n",
      "Online Data is automatically fed into\n",
      "\n",
      "Processing the CPU as soon as it becomes\n",
      "\n",
      "available, Used for continuous\n",
      "processing of data.\n",
      " The data integration process is one of the\n",
      "main components in the overall data management\n",
      "process, employed with increasing frequency as big\n",
      "data integration and the need to share existing data\n",
      "continues to grow. * Streaming Data Integration a real-time data\n",
      "integration method in which different streams\n",
      "of data are continuously integrated and fed\n",
      "into analytics systems and data stores.\n",
      "\n",
      " What is Big Data Integration?\n",
      "\n",
      "fe\n",
      "\n",
      "Big data analytics platforms require scalability\n",
      "and high performance, emphasizing the need for\n",
      "a common data integration platform that supports\n",
      "profiling and data quality, and drives insights by\n",
      "providing the user with the most complete and up-\n",
      "to-date view of their enterprise.\n",
      "\n",
      " 4, Data transformation: Being able to quick!\n",
      "\n",
      "change the way data is summarized and presen'®\n",
      "\n",
      "enables business analysts and executives to quick!\n",
      "consider different perspectives and views of dat\n",
      "Monarch make it easy to package your clean a”\n",
      "blended data for insightful reporting you ©\n",
      "confidently share with the rest of your organizatio\"\n",
      "\n",
      "nN\n",
      "\n",
      "Processing Big Data The methods that can be followed for better Data\n",
      "\n",
      "Preparation and analysis are given below:\n",
      "\n",
      "Data Binning: In this Data Preparation process\n",
      "sorting of data is performed concerning the values\n",
      "of the neighborhood. Structuring: As the data is gathered from\n",
      "different sources, the data will be present in various\n",
      "shapes and sizes, Therefore, there is a need for\n",
      "structuring the data in a proper format,\n",
      "\n",
      "Cleaning: Cleaning or removing of data should\n",
      "be performed that can degrade the performance of\n",
      "the analysis.\n",
      "\n",
      " They receive\n",
      "the requirements from business people and then they\n",
      "use ETL tools to deliver the data in a required format,\n",
      "Data Wrangling is used to analyze the data that was\n",
      "gathered from different data sources. This approach is used for improving\n",
      "the quality of data and consistency rules so that\n",
      "transformations that are applied to the data could\n",
      "be verified,\n",
      "\n",
      "Publishing: After completing the steps of Data\n",
      "Wrangling, the steps can be documented so that\n",
      "similar steps can be performed for the same kind of\n",
      "data to save time.\n",
      "\n",
      " A data lake is usually a single\n",
      "store of all enterprise data, including raw copies of\n",
      "source system data and transformed data used for\n",
      "tasks such as reporting, visualization, analytics. There are three main data processing methods\n",
      "= manual, mechanical and electronic,\n",
      "\n",
      "Manual Data Processing: In this data proce\n",
      "ssing method, data is processed manually. [42] Data Analytics\n",
      "\n",
      "¢ Data Virtualization data from different\n",
      "\n",
      "“systems are virtually combined to create a\n",
      "\n",
      "unified view rather than loading data into a\n",
      "new repository.\n",
      "\n",
      " Additionally, the\n",
      "\n",
      "[88] Data Analytics\n",
      "\n",
      "validation of migrated data for completeness and\n",
      "the decommissioning of legacy data storage are\n",
      "considered part of the entire data migration process.\n",
      "\n",
      " Data Integration\n",
      "\n",
      "+ Data Transformation\n",
      "\n",
      "* Data Reduction\n",
      "\n",
      "Processing Big Data It is an important step prior to processing and\n",
      "often involves reformatting data, making corrections\n",
      "to data and the combining of data sets to enrich data.\n",
      " Data preparation is often a lengthy undertaking\n",
      "for data professionals or business users, but it is\n",
      "essential as a prerequisite to put data in context\n",
      "in order to turn it into insights and eliminate bias\n",
      "resulting from poor data quality.\n",
      "\n",
      " This is the first step which is\n",
      "implemented in Data Preprocessing, In this step, the\n",
      "primary focus is on handling missing data, noisy\n",
      "data, detection, and removal of outliers minimizing\n",
      "duplication, and computed biases within the data,\n",
      "\n",
      "~ A data lake is a storage re ository\n",
      "\n",
      "vast amount of raw data in its aie Pa es ;\n",
      "structured, semistructured, and unstructured ae\n",
      "The data structure and requirements are not defined\n",
      "until the data is needed. There are\n",
      "many reasons for missing data such as data is\n",
      "not continuously collected, a mistake in data\n",
      "entry, technical problems with biometrics,\n",
      "and much more, which requires proper Data\n",
      "Preparation.\n",
      "\n",
      " + Data Wrangling using Data Wrangler:\n",
      "\n",
      "Data Wrangler is a tool that is used to convert\n",
      "-world data into the structured format.\n",
      " A typical migration might involve data from data warehousing, and data wrangling all may invol\n",
      "mariiGes such data transformation.\n",
      " In this case, Data Preprocessing\n",
      "data is prepared exactly after receiving the data from\n",
      "the data source. + Filling the missing values manually:\n",
      "This is one of the best chosen methods of\n",
      "\n",
      "[48] Data Analytics\n",
      "\n",
      "Data Preparation process, But there is one\n",
      "limitation that when there are large data set,\n",
      "and missing values are significant then, this\n",
      "approach is not efficient as it becomes a time-\n",
      "consuming task,\n",
      "\n",
      "+ Filling using computed values: The missing\n",
      "values can also be occupied by computing\n",
      "mean, mode or median of the observed given\n",
      "values. Data Leakage’\n",
      "effect is minimized by recalculating the required\n",
      "Data Preparation during the cross-validation process\n",
      "The cross-validation process includes featur\n",
      "selection, outlier detection and removal, projectio!\n",
      "methods, scaling of selected features, and more\n",
      "Another solution for better Data Preparation i\n",
      "dividing the complete dataset into a training dat\n",
      "set that is used to train the model and validatio!\n",
      " You can store\n",
      "\n",
      "- your data as-is, without having to first structure the\n",
      "\n",
      "data, and run different types of analytics — from\n",
      "dashboards and visualizations to big data processing,\n",
      "real-time analytics, and machine learning — to guide\n",
      "better decisions.\n",
      "\n",
      "Q.10. Data extraction is the process of collecting or\n",
      "\n",
      "Tetrieving disparate types of data from a variety of\n",
      "Sources, many of which may be poorly organized or\n",
      "\n",
      "Processing Big Data\n",
      "\n",
      "— SSS!\n",
      "\n",
      "completely unstructured. Data integration is the practice of consolidating\n",
      "data from disparate sources into a single dataset with\n",
      "the ultimate goal of providing users with consistent\n",
      "access and delivery of data across the spectrum\n",
      "of Subjects and structure types, and to meet the\n",
      "information needs of all applications and business\n",
      "processes. * Data Wrangling in OpenRefine: OpenRefine\n",
      "is Open source software that provides a\n",
      "friendly Graphical User Interface (GUI)\n",
      "that helps to manipulate the data according\n",
      "to your problem statement and makes Data\n",
      "Preparation process simpler. Whereas data integration involves collecting\n",
      "data from sources outside of an organization for\n",
      "analysis, migration refers to the movement of\n",
      "data already stored internally to different systems.\n",
      " This makes data preparation and integration difficult\n",
      "for organizations that collect data from more than\n",
      "just traditional sources, and provides major hurdles\n",
      "for managing the complete inventory of data at a\n",
      "given time.\n",
      "\n",
      " Data integration supports queries in\n",
      "these enormous datasets, benefiting everything from\n",
      "business intelligence and customer data analytics to\n",
      "data enrichment and real time information delivery.\n",
      "\n",
      " ETL\n",
      "software lets businesses combine data from multiple\n",
      "disparate sources in a single data warehouse so\n",
      "that they can run a combination of queries and\n",
      "get the exact data they need to create interactive\n",
      "visualizations for their businesses.\n",
      "\n",
      " Simple\n",
      "\n",
      "_ data processing operations can be achieved with this\n",
      "\n",
      "Method, It has much lesser errors than manual data\n",
      "processing, but the increase of data has made this\n",
      "method more complex and difficult.\n",
      "\n",
      " To deal with\n",
      "the inconsistent data manually and perform Data\n",
      "Preparation and analysis properly, the data ismanaged\n",
      "using external references and knowledge engineering\n",
      "tools like the knowledge engineering process.\n",
      "\n",
      " Middleware Data Integration: Similar to\n",
      "application-based data integration, middleware\n",
      "allows data integration from legacy systems.\n",
      " But, as the data grew\n",
      "and became very huge, bringing this huge amount of\n",
      "data to the processing unit posed the following issues:\n",
      "\n",
      "* Moving huge data to processing is costly and _\n",
      "\n",
      "deteriorates the network performance.\n",
      "\n",
      " Data processing cycle\n",
      "\n",
      "[40] Data Analytics\n",
      "\n",
      "an nme\n",
      "\n",
      "Generally, there are six main steps in the data\n",
      "processing cycle:\n",
      "\n",
      " Therefore, in the Data Preparation process\n",
      "managing data in different formats Data Wrangling\n",
      "is necessary. Data Locality: Instead of moving data to\n",
      "the processing unit, we are moving the processing\n",
      "unit to the data in the MapReduce Framework. * Data Wrangling using Mr, Data Converter:\n",
      "Mr. Data Converter is a tool that takes Excel\n",
      "file as an input and converts the file into\n",
      "required formats. Partition\n",
      "\n",
      "Function\n",
      "\n",
      "Phases of MapReduce data flow\n",
      "\n",
      "Input reader: The input reader reads the upcoming data and splits it into the data blocks of the appropriate\n",
      "size (64 MB to 128 MB). Data exploration is the initial step in data\n",
      "analysis, where users explore a large data set in\n",
      "an unstructured way to uncover initial patterns,\n",
      "characteristics, and points of interest. First of all, we have to understand\n",
      "what Data Leakage is?-\n",
      "Data Leakage in Machine Learning and Deep\n",
      "\n",
      "Learning: Data Leakage is responsible for the cause\n",
      "_of an invalid Machine Learning/Deep Learning model\n",
      "due to the over-optimization of the applied model.\n",
      " © Change Data Capture identifies data\n",
      "\n",
      "changes in databases in real-time and applies\n",
      "\n",
      "them to a data warehouse or other repositories.\n",
      "\n",
      " Here is a list of data integration techniques\n",
      "that are mostly used by businesses to integrate data\n",
      "from separate data sources. In the modem data marketplace, disparate data\n",
      "sources are largely what -we refer to as unstructured\n",
      "in nature, making up the bulk of “big data” volumes.\n",
      " Data Wrangling is conducted to minimize the\n",
      "effect of Data Leakage while executing the model\n",
      "Suppose one considers the complete data set fo\n",
      "normalization and standardization. Numerous challenges need to be taken care\n",
      "of when integrating data from a data source/lake to\n",
      "qa data warehouse. Extracted data is loaded into a destination the\n",
      "serves as a platform for BI reporting, such as a cloy\n",
      "data warehouse like Amazon Redshift, Microso\n",
      "Azure SQL Data Warehouse, Snowfiake, or Goog|\n",
      "BigQuery. Cluttered & Heterogeneous Data: Another\n",
      "challenge for companies is to declutter data to remove\n",
      "irrelevant fields often found in disparate data sources.\n",
      " Data Wrangling in Tabula: Tabula is a tool that\n",
      "is used to convert the tabular data present in pdf into\n",
      "a structured form of data, i.e., spreadsheet.\n",
      " What are the best Data Preprocessing Tools?\n",
      "Ans.\n",
      "\n",
      "Data Preprocessing in R: Ra framework that\n",
      "consists of various packages that can be used\n",
      "for Data Preprocessing like dplyr ete.\n",
      "\n",
      " Data Preprocessing is a technique that is used\n",
      "to convert the raw data into a clean data set. What is the benefits of Data Wrangling?\n",
      "Ans, Data Wrangling is used to handle the issue of\n",
      "Data Leakage while implementing Machine Learning\n",
      "and Deep Learning. For example: if have weather data when we\n",
      "analyze the data it is observed that data is from one\n",
      "area and so primary focus is on determining patterns.\n",
      "\n",
      " Data\n",
      "may be entered into a database, data warehouse, data\n",
      "repository or application.\n",
      "\n",
      " Mostly\n",
      "real-world data is composed of:\n",
      "\n",
      "+ Inaccurate data (missing data): The reasons for the existence of\n",
      "noisy data could be a technological problem\n",
      "of gadget that gathers data, a human mistake\n",
      "during data entry and much more.\n",
      "\n",
      " :\n",
      "data; Some data lack attribute\n",
      "\n",
      "interest, or\n",
      "\n",
      "ame = or\n",
      "\n",
      "Noisy: Some data contains errors.\n",
      "\n",
      " Data discretization: It is a part of data\n",
      "reduction which contains particular importance\n",
      "especially for numerical data.\n",
      "\n",
      " Heartbeat is referred to a signal used between a\n",
      "data node and NameNode, and between task tracker\n",
      "and job tracker, if the Name node or job tracker does\n",
      "not respond to the signal, then it is considered there\n",
      "is some issues with data node or task tracker.\n",
      "\n",
      "Q.15. In such cases, ETL teams have to manually create a\n",
      "custom process for each new data source, decreasing\n",
      "efficiency and increasing the overall cost.\n",
      "\n",
      "Application-based Integration: Today most\n",
      "ETL platforms such as Astera Centerprise, Talend,\n",
      "Informatica, and many others allow data integration\n",
      "through drag and drop features with no coding\n",
      "requirement. The data processing cycle consists of a series\n",
      "of steps where raw data (input) is fed into a process\n",
      "(CPU) to produce actionable insights (output).\n",
      " Eg: Barcode scanning,\n",
      "\n",
      "Data is broken down into frames\n",
      "\n",
      "and processed using two or more\n",
      "CPUs within a single computer\n",
      "system, Also known as parallel\n",
      "processing,\n",
      "\n",
      "Eg: Weather forecasting\n",
      "\n",
      "Multiprocessing\n",
      "\n",
      "Time-sharing\n",
      "\n",
      "Allocates computer resources and\n",
      "data in time slots to several users\n",
      "simultaneously,\n",
      "\n",
      "Q.3. This allows for quick access\n",
      "and retrieval of information whenever needed, and\n",
      "also allows it to be used as input in the next data\n",
      "processing cycle directly,\n",
      "\n",
      "Type\n",
      "Haieh\n",
      "Processing\n",
      "\n",
      "Data is collected and processed in\n",
      "batches. Use, Task, or Process Performance\n",
      "Data: This broad category of data includes\n",
      "information related to specific tasks or\n",
      "operations. As the simple Data Preparation and\n",
      "analysis methods alone are not feasible for the\n",
      "complex problem statement, Data Wrangling i\n",
      "introduced which simplifies the analysis process ofa\n",
      "complex issue. Data ingestion is the overall process of\n",
      "collecting, transferring, and loading data from\n",
      "one or multiple sources so that it may be analyzed\n",
      "immediately or stored in a database for later use. Integrated data from different\n",
      "patient records and clinics helps doctors in diagnosing\n",
      "medical conditions and diseases by organizing data\n",
      "from different systems into a unified view of useful\n",
      "information from which useful insights can be\n",
      "made. Data preparation is the process of cleaning\n",
      "and transforming raw data prior to processing and\n",
      "analysis. Removing manually: The noisy data can be\n",
      "deleted manually by the human being, but it is\n",
      "a time-consuming Data Preparation process; so\n",
      "mostly this method is not given priority. Customer data integration provides business\n",
      "managers and data analysts with a complete picture\n",
      "of key performance indicators (KPIs), financial\n",
      "risks, customers, manufacturing and supply chain\n",
      "operations, regulatory compliance efforts, and other\n",
      "aspects of business processes.\n",
      "\n",
      " As the data is processed by multiple machines instead\n",
      "of a single machine in parallel th e taken to process the data gets reduced by a tremendous amount as\n",
      "\n",
      "Hadoop MapReduce [57]\n",
      "\n",
      "Slave A\n",
      "\n",
      "Data>+ + Indexing and Ordering: Data can be trans-\n",
      "formed so that it’s ordered logically or to suit a data\n",
      "storage scheme, In relational database management\n",
      "systems, for example, creating indexes can improve\n",
      "performance or improve the management of relation-\n",
      "ships between different tables. .\n",
      "\n",
      " Another method could be the predictive\n",
      "values in Data Preprocessing are that are\n",
      "computed by using any Machine Learning\n",
      "or Deep Learning tools and algorithms, But\n",
      "one drawback of this approach is that it can\n",
      "generate bias within the data as the calculated\n",
      "values are not accurate concerning the\n",
      "observed values.\n",
      "\n",
      " The Map task takes input data and converts\n",
      "it into a data set which can be computed in\n",
      "Key value pair. Organizations that w\n",
      "on-premises data warehouses generally usea -\n",
      "ETL (extract, transform, load) process, in whit\n",
      "data transformation is the middle step. Step 6: Storage: The last step of the data\n",
      "processing cycle is storage, where data and metadata\n",
      "are stored for further use. skip preload transformations and load raw data if\n",
      "the data warehouse, then transform it at query tin\n",
      "— a model called ELT (extract, load, transform).\n",
      " Data transformation is the process of changin\n",
      "the format, structure, or values of data. This\n",
      "library helps the data scientist to deal with\n",
      "complex problems efficiently and makes Data\n",
      "Preparation process efficient.\n",
      "\n",
      " * Extract, Load and Transform data is loaded\n",
      "as is into a big data system and trans-formed\n",
      "at a later time for particular analytics uses.\n",
      "\n",
      "_ It is usually performed in a step-by-step\n",
      "process by a team of data scientists and data engineers\n",
      "in an organization. In this way, Data Wrangling is used for\n",
      "improving the analysis process of complex problems\n",
      "during Data Preparation.\n",
      "\n",
      "Q.13. Data Preprocessing in Rapid Miner: Rapid\n",
      "Miner is ar’ open-source Predictive Analytics\n",
      "Platform for Data Mining process. Data migration is the process of selecting\n",
      "preparing, extracting, and transforming data and\n",
      "permanently transferring it from one computer\n",
      "storage system to another.\n",
      "\n",
      " Data extraction makes it\n",
      "possible to consolidate, process, and refine data so\n",
      "that it can be stored in a centralized location in order\n",
      "to be transformed. Therefore Data Wrangling is used to convert the\n",
      "time series data into the required format of the\n",
      "applied model. Companies that require data from only\n",
      "a few sources regularly hire ETL experts to create\n",
      "manual data integration processes since they are\n",
      "cost-effective and don’t require a paid middleware.\n",
      " In the following diagram, the data 6 stored in\n",
      "Data Nodes | and 3 and are directed to the client node\n",
      "by the name node. [46] Data Analytics -\n",
      "\n",
      "Enrichment and Imputation: Data from\n",
      "different sources can be merged to create\n",
      "denormalized, enriched information. Data Preprocessing in Weka: Weka is a\n",
      "software that contains a collection of Machine\n",
      "Learning algorithms for the Data Mining\n",
      "process. It is designed\n",
      "specially to handle diverse and complex data of any\n",
      "scale, But in the case of ETL, it can handle structured\n",
      "data that was originated from different databases\n",
      "or operating systems. Data integration: Access data from an)\n",
      "source — no matter the origin, format or narrativt\n",
      "and integrating them together. In RDBMS, the database cluster uses the same data In Hadoop, the storage data can be stored inde-\n",
      "files stored in a shared storage. Data processing is the method of\n",
      "collecting raw data and translating it into usable\n",
      "information. Checking the presence of Data Leakage within\n",
      "the applied model: Data Leakage is observed at the\n",
      "time of usage of complex datasets. Enrichment: Extract new features or data from\n",
      "the given data set to optimize the performance of the\n",
      "applied model.\n",
      "\n",
      " One of the foremost use cases for data integration\n",
      "services and solutions is.the management of business\n",
      "and customer data. Data is processed\n",
      "with modern technologies using data processing\n",
      "software and programs. In general, the leakage of data is observed\n",
      "from two primary sources of Machine Learning/\n",
      "Deep Learning algorithms such as feature attributes\n",
      "(variables) and training data set.\n",
      "\n",
      " That’s where a data extraction software\n",
      "comes in handy,\n",
      "\n",
      "conne\n",
      "\n",
      "Problem of Duplicates: Duplicates can ruin\n",
      "even healthy insights and they are hard to fix\n",
      "especially if they exist in an unorganized data format.\n",
      " Even after parsing, web data might arrive\n",
      "in the form of hierarchical JSON or XML files, but\n",
      "need to be translated into row and column data for\n",
      "inclusion in a relational database.\n",
      "\n",
      " The problem with this data integration approach is\n",
      "that it doesn’t work when the data sources increase.\n",
      " Data Wrangling is the process of gathering,\n",
      "selecting, and transforming data to answer an\n",
      "analytical question. So, as you can see in the above image that the\n",
      "data is distributed among multiple nodes where each\n",
      "node processes the part of the data residing on it. Equipped with a codeless environment, these\n",
      "software let users extract data from disparate sources\n",
      "by creating data maps and automating workflows.\n",
      " Data binning, bucketing is a data pre-processing method used to minimize the effects of small observation\n",
      "errors. * Resilient to failure: HDFS has the property\n",
      "with which it can replicate data over the\n",
      "network, so if one node is down or some other\n",
      "network failure happens, then Hadoop takes\n",
      "the other copy of data and use it. Parsing fields\n",
      "out of comma-delimited log data for loading to a\n",
      "relational database is an example of this type of data\n",
      "transformation.\n",
      "\n",
      " Data analysts and data scientists\n",
      "can implement further transformations additively as\n",
      "necessary as individual layers of processing. When data is ingested in real time, each data\n",
      "item is imported as it is emitted by the source. By converting the data\n",
      "into a readable format like graphs, charts, and\n",
      "documents, employees throughout the organization\n",
      "can understand and use the data.\n",
      "\n",
      "Q.2. Produce top-quality data: Cleaning and\n",
      "reformatting datasets ensures that all data used\n",
      "in analysis will be high quality.\n",
      "\n",
      " What are the Data Integration Challenges\n",
      "\n",
      "with Disparate Data Sources?\n",
      "\n",
      "\n",
      " Data warehouses function as a place where\n",
      "\n",
      "unalike data types converge as one, with an end\n",
      "goal of providing fodder for reporting and analytics.\n",
      " Map tasks deal\n",
      "with splitting and mapping of data while Reduce\n",
      "tasks shuffle and reduce the data.\n",
      "\n",
      " Users of this software only have to\n",
      "connect both data sources and they can then extract\n",
      "data to the staging area. Performing transformations in an\n",
      "on-premises data warehouse after loading,\n",
      "or transforming data before feeding it into\n",
      "applications, can create a computational\n",
      "burden that slows down other operations.\n",
      " Properly formatted and validated data\n",
      "improves data quality and protects applications\n",
      "from potential landmines such as null values,\n",
      "unexpected duplicates, incorrect indexing, and\n",
      "incompatible formats.\n",
      " Metadata tells some of the story, but in the example of\n",
      "the ever-expanding shape and size of data collection,\n",
      "often times the real substance of data within an\n",
      "enterprise is not well known or can be lost.\n",
      "\n",
      " Users can\n",
      "edit, add, or delete information in this virtualized\n",
      "layer and then load the final data to a data warehouse.\n",
      " Data transformation facilitates compatibility\n",
      "between applications, systems, and types of\n",
      "data. Data analysts without appropriate subject\n",
      "matter expertise are less likely to notice typos\n",
      "or incorrect data because they are-less familiar\n",
      "with the range of accurate and permissible\n",
      "values. I think cluttered data in this case refers to data with\n",
      "a lot of “noise” in it that obscures analysis at the\n",
      "endpoint. The first step\n",
      "in putting data extraction to work for you is to identify\n",
      "the kinds of data you'll need. When\n",
      "data is ingested in batches, data items are imported in\n",
      "discrete chunks at periodic intervals of time.\n",
      "\n",
      " Data cleaning: Manual data prep is error-\n",
      "prone, time-consuming and costly. Omitted data might include\n",
      "numerical indexes in data intended for graphs and\n",
      "dashboards or records from business regions that\n",
      "aren’t of interest in a particular study.\n",
      "\n",
      " Therefore, certain steps are\n",
      "executed to convert the data into a small clean data\n",
      "set. There are challenges to transforming data\n",
      "effectively: 2\n",
      "\n",
      "+ Data transformation can be expensive. [45]\n",
      "\n",
      "transformations should include things like data type\n",
      "conversion and flattening of hierarchical data. Disparate data is made up of any data that are\n",
      "unalike and are distinctly different. Poor Quality Data: Another major challenge for\n",
      "companies is of poor-quality data. SO ‘ Data transformation may be construct\n",
      "at (adding, copying, and replicating data), destrutl\" ?\n",
      " ; Processes such as data integration, data migrati?\n",
      " The Leakage of data from test dataset to the\n",
      "training data set.\n",
      "\n",
      " Sometimes it is not\n",
      "plausible to physically integrate data from specific\n",
      "\n",
      "data sources. [47]\n",
      "\n",
      "Minimizing Data Leakage\n",
      "\n",
      "Recalculation of required\n",
      "data using cross-validation\n",
      "\n",
      "Dividing the Dataset\n",
      "\n",
      "Q.18. In this initial transformations, Data\n",
      "Cleaning or any aggregation of data is performed.\n",
      " Transformation: Match data to its new forms,\n",
      "ensure that metadata reflects the data in each\n",
      "field.\n",
      "\n",
      " They can then extract data,\n",
      "edit it, and load it on the destination data warehouse.\n",
      "\n",
      " Data transformation serves many functions\n",
      "within the data analytics stack.\n",
      "\n",
      " The changes in the virtualization layer don’t alter the\n",
      "data in the data sources.\n",
      "\n",
      " Data Preprocessing steps are performed before\n",
      "the Data Wrangling. With the advancement in the technology and\n",
      "generation of data, data is collected from various\n",
      "sources. For dat\n",
      "analytics projects, data may be transformed at tw\n",
      "stages of the data pipeline. Machine Learning: A Machine Learning\n",
      "algorithm can be executed for the smoothing of data\n",
      "during Data Preprocessing. Filtering, Aggregation, and Summarization:\n",
      "Data transformation is often concerned with whittling\n",
      "data down and making it more manageable. By preparing data, we actually prepare the\n",
      "miner so that when using prepared data, the\n",
      "miner produces better models faster.\n",
      "\n",
      " Data Preprocessing is carried out to remove\n",
      "the cause of unformatted real-world data which we\n",
      "discussed above. Data Preprocessing is necessary because of\n",
      "the presence of unformatted real-world data. First of all, let’s explain how missing\n",
      "data can be handled during Data Preparation. Altair Monarch is programmed with ove!\n",
      "80 pre-built data preparation functions to speed uy\n",
      "arduous data cleansing projects.\n",
      "\n",
      " Translation and Mapping: Some of the most\n",
      "basic data transformations involve the mapping and\n",
      "translation of data. How Data Wrangling improves Data\n",
      "Analytics?\n",
      "\n",
      "Ans. * Data Wrangling in R: R is an important\n",
      "programming language for the data scientist.\n",
      " Discuss the difference between Data\n",
      "Preparation vs Data Wrangling.\n",
      "\n",
      "Ans. Transforming data yields several benefits :\n",
      "\n",
      ": Data is transformed to make it better-\n",
      "\n",
      "organized. Databases, data warehouses, and data lakes are all\n",
      "governed in unique ways. These data blocks are used to store data.\n",
      " + The presence of noisy data (erroneous data\n",
      "and outliers): Leakage of future data into the past data.\n",
      "\n",
      " Hadoop MapReduce [55]\n",
      "\n",
      "3 Task Trackers on 3 Datanodes\n",
      "\n",
      "Job Tracker on Namenode\n",
      "\n",
      "Task\n",
      "Ee Tracker\n",
      "\n",
      "Status\n",
      "Update\n",
      "\n",
      "Job is being submitted\n",
      "\n",
      "Status\n",
      "Update\n",
      "\n",
      "How Hadoop MapReduce Works\n",
      "A job is divided into multiple tasks which are then run onto multiple data nodes in a cluster.\n",
      " Best practices for\n",
      "real-time data integration address its dirty, moving,\n",
      "and temporal nature: more stimulation and testing is\n",
      "required upfront, real-time systems and applications\n",
      "should be adopted, users should implement paraliel\n",
      "and coordinated ingestion engines. However, specific systems function in different ways,\n",
      "and depending on how the source stores and transfers\n",
      "‘its volume, steps need to be taken upon arrival to\n",
      "ensure data quality. For example when we want to use\n",
      "the particular feature for performing Predictive\n",
      "Analysis, but that specific feature is not present at\n",
      "the time of training of dataset then data leakage will\n",
      "be introduced within the model. The phase that controls the partitioning of\n",
      "intermediate MapReduce output keys is known as\n",
      "a partitioner, The process also helps to provide the\n",
      "Input data to the reducer. Even the tools to process the data are\n",
      "often on the same servers, thus reducing the\n",
      "processing time. Execution of individual task is then to look after by task tracker, which resides on every data node\n",
      "\n",
      "executing part of the job.\n",
      " In that case, the\n",
      "cross-validation is performed to estimate the models\n",
      "performance leads to the beginning of data leakage\n",
      "Another problem is also observed that the test mode}\n",
      "is also included for feature selection while executing\n",
      "each fold of cross-validation, which further generates\n",
      "bias during performance analysis. While nameNode\n",
      "handles the metadata of les in HDFS ;\n",
      "\n",
      "‘The main difference between NameNode ang\n",
      "DataNode in Hadoop is that the NameNode is the\n",
      "master node in HDFS that manages the file system\n",
      "metadata while the DataNode is a SlaveNode\n",
      "in HDFS that stores the actual data as instructed by\n",
      "the NameNode. Whether the source is a database or ag\n",
      "platform, the data extraction process involves\n",
      "following steps:\n",
      "1. It is\n",
      "a low-cost method and requires little to no tools, but\n",
      "produces high errors, high labor costs and lots of time,\n",
      "\n",
      "Mechanical Data Processing: In\n",
      "the traditional system, we used to bring data to the\n",
      "processing unit and process it. It is a specific compressed binary\n",
      "le format which is optimized for passing data\n",
      "t een the output of one MapReduce job to the\n",
      "“some other MapReduce job.\n",
      "\n",
      "Q.18. [58] Data Analytics\n",
      "\n",
      "ee\n",
      "\n",
      "The purpose behind GFS was the ability to store\n",
      "and access large files, and by large I mean files that\n",
      "can’t be stored on a single hard drive. However, data integration\n",
      "software allows companies to complete these tasks\n",
      "in a matter of minutes thanks to simple GUI and the\n",
      "ability to add transformation and validation rules.\n",
      "\n",
      " This can be achieved\n",
      "through a variety of data integration techniques,\n",
      "including: s E\n",
      "\n",
      "¢ Extract, Transform and Load copies of\n",
      "datasets from disparate sources are gathered\n",
      "together, harmonized. Then, the reducer aggregates those intermediate\n",
      "\n",
      "data tuples (intermediate key-value pair) intoa\n",
      "\n",
      "smaller set of tuples or key-value pairs which\n",
      "\n",
      "a\n",
      "\n",
      "ts. Effective data acquisition and integration also\n",
      "improves claims processing accuracy for medical\n",
      "insurers and ensures a consistent and accurate record\n",
      "of patient names and contact information. Data extraction is a powerful and adaptable\n",
      "process that can help you gather many types of\n",
      "information relevant to your business. A set of instructions is given\n",
      "to the software to process the data and yield output.\n",
      " The\n",
      "entire process of data collection, filtering, sorting,\n",
      "calculation and other logical operations are all done\n",
      "with human intervention without the use of any\n",
      "other electronic device or automation software. But, this method should not\n",
      "be performed at the time when the number of\n",
      "missing values is immense or when the pattern\n",
      "of data is related to the unrecognized primary\n",
      "root of the cause of the statement problem.\n",
      "\n",
      " —\n",
      "\n",
      "Translation converts data from formats used\n",
      "in one system to formats appropriate for a different\n",
      "system. Big Data Integration services employ real-time\n",
      "integration techniques. A map reads data from an input location, and\n",
      "outputs a key value pair according to the input type.\n",
      " In other\n",
      "words, whenever the data is gathered from different\n",
      "sources it is collected in taw format which is not\n",
      "feasible for the analysis. So, MapReduce is based on Divide and Conquer paradigm which\n",
      "helps us to process the data using different machines. Every node gets a part of the data to process\n",
      "and therefore, there is no chance of a node\n",
      "getting overburdened.\n",
      "\n",
      " The first data\n",
      "\n",
      "processing cycle’s output can be stored and fed as the\n",
      "input for the next cycle.\n",
      "\n",
      " Make better business decisions: Higher\n",
      "quality data that can be processed and analyzed\n",
      "more quickly and efficiently leads to more\n",
      "\n",
      "timely, efficient and high quality business\n",
      "decisions.\n",
      "\n",
      " Shufiling and Sorting: The data are shuffled between/within nodes so that it moves out from the map and\n",
      "get ready to process for reduce function. Step 5: Output: The data is finally transmitted\n",
      "and displayed to the user in a readable form like\n",
      "graphs, tables, vector files, audio, video, documents,\n",
      "etc. Explain its working\n",
      "components,\n",
      "\n",
      "MapReduce is a programming framework\n",
      "that allows us to perform distributed and parallel\n",
      "processing on large data sets in a distributed\n",
      "environment.\n",
      "\n",
      " Three\n",
      "different steps can be executed which are given\n",
      "below -\n",
      "\n",
      "* Ignoring the missing record: It is the\n",
      "simplest and efficient method for handling\n",
      "the missing data. Cost Effective: Hadoop is open source and\n",
      "uses commodity hardware to store data so it\n",
      "really cost effective as compared to traditional\n",
      "relational database management system.\n",
      "\n",
      " The\n",
      "\n",
      "cost is dependent on the specific infrastructure, .\n",
      "software, and tools used to process data. A data preparation tool could be used in this scenario to identify an incorrect number of unique values (in\n",
      "the case of virus, a unique count greater than suitable number in case of covid would raise a flag, as there are\n",
      "only few names aligned with virus). Some of\n",
      "these are legacy systems and integrating them into\n",
      "modern data warehouses is a complex process.\n",
      "\n",
      " Processing takes time as the data is processed\n",
      "\n",
      "by asingle unit which becomes the bottleneck.\n",
      "\n",
      " Moving data to the Pocessing unit\n",
      "(Traditional Approach)\n",
      "\n",
      "Slave A\n",
      "\n",
      "Slave B\n",
      "\n",
      "e)\n",
      "\n",
      "Slave E\n",
      "\n",
      "QO\n",
      "\n",
      "Slave D\n",
      "\n",
      "Master\n",
      "\n",
      "Slave C\n",
      "\n",
      "1. The processing time is reduced as all the\n",
      "nodes are working with their part of the data\n",
      "in parallel.\n",
      "* MapReduce is a software framework and\n",
      "programming model used for processing huge\n",
      "amounts of data. So, the first is the map job, where a block of\n",
      "data is read and processed to produce key-\n",
      "value pairs as intermediate outputs.\n",
      "\n",
      " It provides\n",
      "efficient tools for performing the exact Data\n",
      "Preprocessing process.\n",
      "\n",
      " Also known as data cleaning or\n",
      "“munging,” legend has it that this wrangling costs\n",
      "analytics professionals as much as 80% of their time,\n",
      "paving only 20% for exploration and modeling.\n",
      " Increased access to data means less manu\n",
      "work, faster insights and faster time to value realize\n",
      "by your organization.\n",
      "\n",
      " This output can be stored and further processed\n",
      "in the next data processing cycle.\n",
      "\n",
      " Explain different Types of Data Processing\n",
      "models.\n",
      "\n",
      " The selection of the mode\n",
      "is made by looking at the results of the test data s¢\n",
      "in the cross-validation process. * Data Wrangling in CSVKit: CSVKit is a\n",
      "toolkit that provides the facility of conversion\n",
      "of CSV files into different formats like CSV\n",
      "to JSON, JSON to CSV, and much more. For example, Age = 56, Birthdate = ’04-)5.\n",
      "1995’\n",
      "\n",
      "Q.25, Explain the Data Preparation steps with the\n",
      "help of example.\n",
      " Data Wrangling technology is used by business\n",
      "analysts, users engaged in business, and managers,\n",
      "On the other hand, ETL (Extract, Transform, and\n",
      "Load) is employed by IT Professionals. thus are very\n",
      "useful for performing large scale data analysis using\n",
      "multiple machines in the cluster.\n",
      "\n",
      " We specify the names\n",
      "of Mapper and Reducer Classes long with”\n",
      "data types and their respective job names. Another concept\n",
      "is that when time-series data has to be handled\n",
      "every algorithm is executed with different aspects.\n",
      " Data processing is crucial for organizations\n",
      "to create better business strategies and increase\n",
      "their competitive edge. Long or freeform fields may be split into multiple\n",
      "columns, and missing values can be imputed or\n",
      "corrupted data replaced as a result of these kinds of\n",
      "transformations.\n",
      "\n",
      " —o\n",
      "evaluation |\n",
      "ta mining ; —_—” |\n",
      "\n",
      "Dal Knowledge\n",
      "\n",
      "cele Patterns\n",
      "\n",
      "<a\n",
      "Prepared\n",
      "Data\n",
      "\n",
      "on\n",
      "Cleaned Fransrormat\n",
      "\n",
      "cleaning\n",
      "OS oom\n",
      "ia)\n",
      "\n",
      " It will first talk to the name\n",
      "node and the name node will direct it to the right data\n",
      "node. Distributed file systems store data\n",
      "across a large number of servers. Fix errors quickly: Data preparation helps\n",
      "catch errors before processing. Shuffling is the process of transferring data\n",
      "\n",
      "from Mapper to Reducer. For example,\n",
      "copying data from Cobol to delimited files is not\n",
      "possible unless both Cobol and delimited files\n",
      "connectors are available. Encryption of private data is a requirement in many\n",
      "industries, and systems can perform encryption at\n",
      "multiple levels, from individual database cells to\n",
      "entire records.or fields. Data used for multiple purposes may\n",
      "need to be transformed in different ways.\n",
      "\n",
      " Once input reads the data, it generates the corresponding key-value pairs. Data extraction is the first step in both ETL\n",
      "(extract, transform, load) and ELT (extract, load,\n",
      "transform) processes. The raw data is collected,\n",
      "filtered, sorted, processed, analyzed, stored, and then\n",
      "presented in a readable format.\n",
      "\n",
      " The concept of\n",
      "Data Preparation steps performed before applying\n",
      "any iterative model and will be executed once in\n",
      "the project. In simple words, the complex data\n",
      "is transformed into a usable format for performing\n",
      "analysis on it. It\n",
      "makes the process of data wrangling easy.\n",
      "\n",
      " Reading a File in HDFS: Let's say an\n",
      "application is running on a client node that needs to\n",
      "access data from HDFS. Data preparation example: There are multiple values that are commonly used to represent the virus. RecordReader: It interacts with the Inpy\n",
      "split and converts the obtained data in th\n",
      "form of Key-Value Pairs. This is a framework which\n",
      "helps Java programs to do the parallel\n",
      "computation on data using key value pair.\n",
      " The original data values are divided into small intervals known as bins and then they are replaced by\n",
      "a general value calculated for that bin. + Cost-effective: Allows data storage and\n",
      "processing at affordable prices.\n",
      "\n",
      " Explain the different types of Data\n",
      "Extraction. ’ Data 1 Leakage is ihe term used whe t d\n",
      "\n",
      "of the model. It is able to process terabytes\n",
      "of data in minutes and peta-bytes in hours.\n",
      "\n",
      " The problem is that integrating disparate data\n",
      "sources is a cumbersome task. Instead gy\n",
      "rewriting files, GFS is optimized towards appending\n",
      "data to existing files in the system.\n",
      "\n",
      " Advantages of Hadoop ‘\n",
      "\n",
      "+ Fast: In HDFS the data distributed over the\n",
      "cluster and are mapped which helps in faster\n",
      "retrieval. On the other hand, Data Wrangling\n",
      "is performed during the iterative analysis and\n",
      "model building. For example, someone working on\n",
      "medical data who is unfamiliar with relevant\n",
      "terms might fail to flag disease names that\n",
      "should be mapped to a singular value or notice\n",
      "misspellings.\n",
      "\n",
      " © 2; e\n",
      "\n",
      "There are 2 methods of dividing data into bins.\n",
      "\n",
      " Hadoop brings different\n",
      "data types together in one place, but does not\n",
      "guarantee any substantive forms of organization.\n",
      " This\n",
      "includes casting and converting data types for\n",
      "compatibility, adjusting dates and times with offsets\n",
      "and format localization, and renaming schemas, :\n",
      "tables, and columns for clarity.\n",
      "\n",
      "Q.16. ee\n",
      "2 ped\n",
      "Data warehouse Reportdashboard\n",
      "0G\n",
      "Business applications\n",
      "\n",
      "+ If you use a cloud-based data warehouse,\n",
      "you can do the transformations after loading\n",
      "because the platform can scale up to meet\n",
      "demand.\n",
      "\n",
      " The sorting operation is performed on input data for Reduce function. Explain the Data Processing Cycle and its\n",
      "steps.\n",
      "\n",
      " Now, MapReduce allows us-to overcome the\n",
      "above issues by bringing the processing unit to the\n",
      "data. Unless you have\n",
      "connectors of each disparate source available, you\n",
      "will need to manually take all the data. This type of data is not useful for getting\n",
      "any insights because it is simply not in any specific\n",
      "format. Extracting data from these legacy\n",
      "systems is not possible without the right connectors\n",
      "and that’s what the middleware ETL software offer.\n",
      " By performing filtering, grouping, and\n",
      "selecting appropriate data accuracy and performance\n",
      "of the model could be increased. A self-driving car uses real-time data from\n",
      "sensors to detect if there are pedestrians and\n",
      "other cars on the road.\n",
      "\n",
      " It is very cost-effective to move processing\n",
      "unit to the data.\n",
      " Often, data is gathered in a non-rigid or\n",
      "led mar Jarge bi “or analysis, this\n",
      "CC e narrowed down.\n",
      "\n",
      " Data is processed\n",
      "mechanically through the use of devices and\n",
      "machines, These can include simple devices such as\n",
      "calculators, typewriters, printing press, ete. The DataNodes store and retrieve blocks\n",
      "as instructed by the nameNode.\n",
      "\n",
      "Metadata (Name, replicas, ...):\n",
      "Ihome/foo/data, 3,...\n",
      "\n",
      "All DataNodes continuously communicate with\n",
      "the NameNode. Monarch excels #\n",
      "intelligently and automatically extracting data from\n",
      "complex unstructured and semi-structured source:\n",
      "like PDFs. Here are some\n",
      "real-life examples of data processing :\n",
      "\n",
      "* Companies will typically migrate data when\n",
      "implementing a new system or merging to a new\n",
      "environment. The major tasks in data preparation are a\n",
      "\n",
      "follows:\n",
      "\n",
      "1. In brief, NameNode controls and\n",
      "manages a single or multiple data nodes,\n",
      "\n",
      "Q.12. Data can be streamed in-real time or ingested in\n",
      "batches. Therefore, data is converted\n",
      "\n",
      "to the proper feasible format before applying any\n",
      "model to it. How to Integrate Disparate Data Sources\n",
      "with ETL Software?\n",
      "\n",
      "Ans. An arrangement of networked computers in\n",
      "which data processing capabilities are spread across\n",
      "the network. It helps companies easily get\n",
      "layer of data to extract information. Today, me\n",
      "organizations use cloud-based data warehoust\n",
      "which can scale compute and storage resoutt\n",
      "with latency measured in seconds or minutes. This has a smoothing effect on the input data and may also reduce the\n",
      "\n",
      "chances of over fitting in case of small datasets.\n",
      " Businesseg need insights to make the right\n",
      "decisions and insights require data to become\n",
      "accurate. Migration techniques are often\n",
      "performed by a set of programs or automated scripts\n",
      "that automatically transfer data. It is currently used for analytical and for BIG DATA\n",
      "processing.\n",
      "\n",
      " [41]\n",
      "\n",
      "TT CHEESE acs Sa Pa TR ma a cm I ES, AA EA\n",
      "\n",
      "Electronic Data Processing: What is Distributed Data Processing (DDP)?\n",
      "\n",
      "46280\n",
      "87441\n",
      "− 162 = 8 \n",
      "162 = μ – σ \n",
      "The area between 162 and 186 is the area from μ – σ → μ + 2σ\n",
      "\n",
      "Percentage from μ – σ → μ + 2σ The area between 154 and 178 is the area from μ – 2σ → μ + σ\n",
      "\n",
      "Percentage from μ − 2σ → μ + σ = 13·5% + 68% = 81·5%\n",
      "Question 6. Mean = 170 cm (μ); Standard deviation = 8 cm (σ) \n",
      "(i) greater than 186 cm \n",
      "186 = 170 + 16 \n",
      "170 + 2(8) \n",
      " 170 − 154 = 16 \n",
      "= 2(8) \n",
      "154 = μ − 2σ \n",
      "178 − 170 = 8 \n",
      "178 = μ + σ \n",
      " A) the 10 phones sampled and tested \n",
      "B) all the phones produced during the day in question \n",
      "C) the 4% of the phones that are defective \n",
      "D) the 10 responses: defective or not defective\n",
      "(Ans. B Explain population in detail along with this)\n",
      "Question5. This means that greater than 186 cm is the area above μ + 2σ\n",
      "\n",
      "Percentage above 2σ The heights of male students is normally distributed with a mean of 170 cm and a \n",
      "standard deviation of 8 cm. = 2·35% + 0·15% (from graph) = 2·5%\n",
      "(ii) between 162 cm and 186 cm \n",
      "170 = 68% + 13·5% (From graph) = 81·5% \n",
      "(iii) between 154 cm and 178 cm. \n",
      "\n",
      "998\n",
      "2248\n",
      "Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.\n",
      "\n",
      "\n",
      " Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.\n",
      "\n",
      " Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\n",
      " Vestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. \n",
      " Vestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Lorem ipsum \n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Mauris diam felis, vulputate ac suscipit et, iaculis non est. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Mauris diam felis, vulputate ac suscipit et, iaculis non est. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Cras aliquam est ac eros varius, id iaculis dui auctor. Cras aliquam est ac eros varius, id iaculis dui auctor. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Nulla iaculis tellus sit amet mauris tempus fringilla.\n",
      " Nulla iaculis tellus sit amet mauris tempus fringilla.\n",
      " Curabitur semper arcu ac ligula semper, nec luctus nisl blandit. Duis pretium neque ligula, et pulvinar mi placerat et. Curabitur semper arcu ac ligula semper, nec luctus nisl blandit. Duis pretium neque ligula, et pulvinar mi placerat et. Nulla nec nunc sit amet nunc posuere vestibulum. Suspendisse neque nisl, fringilla at iaculis scelerisque, ornare vel dolor. Nulla nec nunc sit amet nunc posuere vestibulum.\n",
      "2256\n",
      "5882\n",
      "Vivamus  dapibus  sodales  ex,  vitae  malesuada  ipsum  cursus\n",
      "convallis. Etiam  id  mauris  vitae  orci  maximus  ultricies.   Pellentesque  fringilla  mollis  efficitur.   Pellentesque  fringilla  mollis  efficitur.   Ut\n",
      "et  pulvinar  nunc.   Ut\n",
      "et  pulvinar  nunc.   Cras  fringilla  ipsum\n",
      "magna, in fringilla dui commodo a.\n",
      " Pellentesque fermentum nisl vitae\n",
      "fringilla  venenatis.   Lorem\n",
      "2 Cras fringilla ipsum magna, in fringilla dui commodo\n",
      "a.Ipsum\n",
      "3 Aliquam erat volutpat. Ut ac lorem\n",
      "sed turpis imperdiet eleifend sit amet id sapien. Ut ac lorem\n",
      "sed turpis imperdiet eleifend sit amet id sapien. Ut ac dolor vitae odio interdum\n",
      "condimentum.   Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\n",
      " Maecenas tincidunt est efficitur\n",
      "ligula euismod, sit amet ornare est vulputate.\n",
      " Lorem ipsum \n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing \n",
      "elit. Nullam  venenatis  commodo\n",
      "imperdiet. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Nullam  venenatis  commodo\n",
      "imperdiet. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Nunc vulputate neque vitae justo facilisis, non condimentum ante\n",
      "sagittis. \n",
      " Nunc vulputate neque vitae justo facilisis, non condimentum\n",
      "ante sagittis. Vestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Vestibulum neque massa, scelerisque sit amet ligula eu, congue\n",
      "molestie mi. Pellentesque\n",
      "scelerisque fermentum erat, id posuere justo pulvinar ut. Pellentesque\n",
      "scelerisque fermentum erat, id posuere justo pulvinar ut. Maecenas sed egestas nulla, ac condimentum orci.   Cras aliquam\n",
      "est ac eros varius, id iaculis dui auctor. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Cras aliquam\n",
      "est ac eros varius, id iaculis dui auctor. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Mauris diam felis,\n",
      "vulputate ac suscipit et, iaculis non est. Mauris diam felis,\n",
      "vulputate ac suscipit et, iaculis non est. Cras justo mi, porttitor quis mattis vel,\n",
      "ultricies ut purus. Cras justo mi, porttitor quis mattis vel,\n",
      "ultricies ut purus.\n",
      "2235\n",
      "6255\n",
      "Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.\n",
      "\n",
      "\n",
      " Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.\n",
      "\n",
      "\n",
      "\n",
      " Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.\n",
      " Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\n",
      " Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\n",
      " Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. \n",
      " Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Vestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Vestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Lorem ipsum \n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras aliquam est ac eros varius, id iaculis dui auctor. Cras aliquam est ac eros varius, id iaculis dui auctor. Cras aliquam est ac eros varius, id iaculis dui auctor. Mauris diam felis, vulputate ac suscipit et, iaculis non est. Duis pretium neque ligula, et pulvinar mi placerat et. Mauris diam felis, vulputate ac suscipit et, iaculis non est. Duis pretium neque ligula, et pulvinar mi placerat et. Duis pretium neque ligula, et pulvinar mi placerat et. Nulla iaculis tellus sit amet mauris tempus fringilla.\n",
      " Nulla iaculis tellus sit amet mauris tempus fringilla.\n",
      " Suspendisse neque nisl, fringilla at iaculis scelerisque, ornare vel dolor. Suspendisse neque nisl, fringilla at iaculis scelerisque, ornare vel dolor. Suspendisse neque nisl, fringilla at iaculis scelerisque, ornare vel dolor. Duis vehicula mi vel mi pretium, a viverra erat efficitur. Duis vehicula mi vel mi pretium, a viverra erat efficitur. Duis vehicula mi vel mi pretium, a viverra erat efficitur. In vel metus congue, pulvinar lectus vel, fermentum dui.\n",
      "3156\n",
      "8006\n"
     ]
    }
   ],
   "source": [
    "for i in file_list:\n",
    "    path = r\"C:\\Users\\acer\\Desktop\\test\\{}\".format(i)\n",
    "    def get_file_extension(path):\n",
    "        return splitext(path)[1][1:]  # Remove the leading dot\n",
    "    # Example usage:\n",
    "    extension = get_file_extension(path)\n",
    "    #print(\"File extension:\", extension)\n",
    "    if(extension == 'docx'):\n",
    "        # Load the Word document\n",
    "        doc = Document(path)\n",
    "        # Read the content of the document\n",
    "        content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    else:\n",
    "        def check_pdf_content(pdf_path):\n",
    "            has_text = False\n",
    "            has_images = False\n",
    "            with open(pdf_path, 'rb') as pdf_file:\n",
    "                pdf_reader = PdfReader(pdf_file)\n",
    "                # Check if the PDF contains text\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text.strip():  # Check if the extracted text is not empty\n",
    "                        has_text = True\n",
    "                        break\n",
    "                # Check if the PDF contains images\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    xObject = page['/Resources']['/XObject'].get_object()\n",
    "                    if xObject is not None:\n",
    "                        for obj in xObject:\n",
    "                            if xObject[obj]['/Subtype'] == '/Image':\n",
    "                                has_images = True\n",
    "                                break\n",
    "            return has_text, has_images\n",
    "        if __name__ == \"__main__\":\n",
    "            text_present, images_present = check_pdf_content(path)\n",
    "\n",
    "            if text_present:\n",
    "                # If the page has text, extract it directly\n",
    "                with open(path, 'rb') as pdf_file:\n",
    "                    # Create a PDF reader object\n",
    "                    pdf = PdfReader(pdf_file)\n",
    "                    # Get the number of pages in the PDF\n",
    "                    num_pages = len(pdf.pages)\n",
    "                    # Extract text from all pages and store it in the 'text' variable\n",
    "                    content = ''\n",
    "                    for page_num in range(num_pages):\n",
    "                        page = pdf.pages[page_num]\n",
    "                        content += page.extract_text()\n",
    "                #print(\"The PDF contains text.\")\n",
    "\n",
    "            elif images_present:\n",
    "                    # Open the PDF file and read its content using PyPDF2\n",
    "                    with open(path, 'rb') as pdf_file:\n",
    "                        pdf_reader = PdfReader(pdf_file)\n",
    "                        # Iterate through all pages\n",
    "                        for page_num in range(len(pdf_reader.pages)):\n",
    "                            # Convert the page to an image using pdf2image\n",
    "                            images = convert_from_path(path, first_page=page_num + 1, last_page=page_num + 1, poppler_path = r\"C:\\Users\\acer\\Downloads\\Release-23.07.0-0\\poppler-23.07.0\\Library\\bin\")\n",
    "                            # Save each image to the output folder\n",
    "                            for idx, image in enumerate(images):\n",
    "                                #image_path = join(f\"page{page_num + 1}_image{idx + 1}.jpg\")\n",
    "                                #img = image.save(image_path, 'JPEG')\n",
    "                                extracted_text = pytesseract.image_to_string(Image.open(join(f\"page{page_num + 1}_image{idx + 1}.jpg\")))\n",
    "                                content += extracted_text + \"\\n\"\n",
    "\n",
    "    #print(content)\n",
    "    doc = nlp(content)\n",
    "    tokens = [token.text for token in doc]\n",
    "    #print(tokens)\n",
    "    punctuation = punctuation + '\\n'\n",
    "    word_frequencies = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in stopwords:\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "                #print(word_frequencies)\n",
    "                max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word]/max_frequency\n",
    "        #print(word_frequencies)\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    #print(sentence_tokens)\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    select_length = int(len(sentence_tokens)*0.3)\n",
    "    summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
    "    final_summary = [word.text for word in summary]\n",
    "    summary = ' '.join(final_summary)\n",
    "    print(summary)\n",
    "    print(len(summary))\n",
    "    print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc3cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
